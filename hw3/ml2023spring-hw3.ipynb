{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## HW3 Image Classification\n#### Solve image classification with convolutional neural networks(CNN).\n#### If you have any questions, please contact the TAs via TA hours, NTU COOL, or email to mlta-2023-spring@googlegroups.com","metadata":{}},{"cell_type":"code","source":"# check GPU type.\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:47.670418Z","iopub.execute_input":"2023-03-20T18:02:47.670793Z","iopub.status.idle":"2023-03-20T18:02:48.750438Z","shell.execute_reply.started":"2023-03-20T18:02:47.670759Z","shell.execute_reply":"2023-03-20T18:02:48.748727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import Packages","metadata":{}},{"cell_type":"code","source":"_exp_name = \"sample\"","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:48.753513Z","iopub.execute_input":"2023-03-20T18:02:48.753986Z","iopub.status.idle":"2023-03-20T18:02:48.761637Z","shell.execute_reply.started":"2023-03-20T18:02:48.753901Z","shell.execute_reply":"2023-03-20T18:02:48.759439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary packages.\nimport numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport torch.nn as nn\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom PIL import Image\n# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\nfrom torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\nfrom torchvision.datasets import DatasetFolder, VisionDataset\n# This is for the progress bar.\nfrom tqdm.auto import tqdm\nimport random\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:48.763614Z","iopub.execute_input":"2023-03-20T18:02:48.764837Z","iopub.status.idle":"2023-03-20T18:02:48.773397Z","shell.execute_reply.started":"2023-03-20T18:02:48.764797Z","shell.execute_reply":"2023-03-20T18:02:48.771928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myseed = 6666  # set a random seed for reproducibility\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(myseed)\ntorch.manual_seed(myseed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(myseed)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:48.777821Z","iopub.execute_input":"2023-03-20T18:02:48.778216Z","iopub.status.idle":"2023-03-20T18:02:48.784993Z","shell.execute_reply.started":"2023-03-20T18:02:48.778186Z","shell.execute_reply":"2023-03-20T18:02:48.783740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transforms","metadata":{}},{"cell_type":"code","source":"# Normally, We don't need augmentations in testing and validation.\n# All we need here is to resize the PIL image and transform it into Tensor.\ntest_tfm = transforms.Compose([\n    transforms.Resize((224, 224)),\n#     transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\n# However, it is also possible to use augmentation in the testing phase.\n# You may use train_tfm to produce a variety of images and then test using ensemble methods\n\ntrain_tfm = transforms.Compose([\n    # Resize the image into a fixed shape (height = width = 128)\n    transforms.RandomResizedCrop((224,224), scale=(0.7, 1.0)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ColorJitter(brightness=(0.7, 1.3), contrast=(\n        0.7, 1.3), saturation=(0.7, 1.3)),\n    transforms.RandomRotation(45),\n#     transforms.Resize((128, 128)),\n    # You may add some transforms here.\n    \n    # ToTensor() should be the last one of the transforms.\n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:48.786580Z","iopub.execute_input":"2023-03-20T18:02:48.787792Z","iopub.status.idle":"2023-03-20T18:02:48.797409Z","shell.execute_reply.started":"2023-03-20T18:02:48.787724Z","shell.execute_reply":"2023-03-20T18:02:48.796337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Datasets","metadata":{}},{"cell_type":"code","source":"class FoodDataset(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,files = None):\n        super(FoodDataset).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n            \n        self.transform = tfm\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        im = self.transform(im)\n        \n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n            \n        return im,label","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:48.798997Z","iopub.execute_input":"2023-03-20T18:02:48.799470Z","iopub.status.idle":"2023-03-20T18:02:48.821339Z","shell.execute_reply.started":"2023-03-20T18:02:48.799432Z","shell.execute_reply":"2023-03-20T18:02:48.817824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FoodDataset_test(Dataset):\n\n    def __init__(self,path,tfm=test_tfm,tfm2=train_tfm,files = None):\n        super(FoodDataset_test).__init__()\n        self.path = path\n        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n        if files != None:\n            self.files = files\n        print(f\"One {path} sample\",self.files[0])\n        self.transform = tfm\n        self.train_transform = tfm2\n  \n    def __len__(self):\n        return len(self.files)\n  \n    def __getitem__(self,idx):\n        fname = self.files[idx]\n        im = Image.open(fname)\n        \n        x=[]\n        y=self.train_transform(im)\n        for i in range(5):\n            x.append(self.train_transform(im))\n        #im = self.data[idx]\n        im = self.transform(im)\n        try:\n            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n        except:\n            label = -1 # test has no label\n        return im,x,label","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:48.827190Z","iopub.execute_input":"2023-03-20T18:02:48.827694Z","iopub.status.idle":"2023-03-20T18:02:48.853407Z","shell.execute_reply.started":"2023-03-20T18:02:48.827649Z","shell.execute_reply":"2023-03-20T18:02:48.850703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n\n            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n\n            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n\n            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n            \n            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n        )\n        self.fc = nn.Sequential(\n            nn.Linear(512*4*4, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 11)\n        )\n\n    def forward(self, x):\n        out = self.cnn(x)\n        out = out.view(out.size()[0], -1)\n        return self.fc(out)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:48.858786Z","iopub.execute_input":"2023-03-20T18:02:48.859240Z","iopub.status.idle":"2023-03-20T18:02:48.876180Z","shell.execute_reply.started":"2023-03-20T18:02:48.859200Z","shell.execute_reply":"2023-03-20T18:02:48.874328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Classifier_VGG19_bn(nn.Module):\n    def __init__( self, features: nn.Module, num_classes: int = 11, init_weights: bool = True, dropout: float = 0.5):\n        super(Classifier_VGG19_bn, self).__init__()\n        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n        # input 維度 [3, 128, 128]\n        \n        self.features = features\n        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 4 * 4, 1024),\n            nn.ReLU(True),\n            nn.Dropout(p=dropout),\n            nn.Linear(1024, 512),\n            nn.ReLU(True),\n            nn.Dropout(p=dropout),\n            nn.Linear(512, num_classes),\n        )\n        if init_weights:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n                    if m.bias is not None:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, nn.BatchNorm2d):\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n                elif isinstance(m, nn.Linear):\n                    nn.init.normal_(m.weight, 0, 0.01)\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:48.878015Z","iopub.execute_input":"2023-03-20T18:02:48.879097Z","iopub.status.idle":"2023-03-20T18:02:48.897625Z","shell.execute_reply.started":"2023-03-20T18:02:48.879057Z","shell.execute_reply":"2023-03-20T18:02:48.895990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VGG(nn.Module):\n    def __init__(self, features, num_classes=11, init_weights=False, dropout: float = 0.5):\n        super(VGG, self).__init__()\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 4 * 4, 1024),\n            nn.ReLU(True),\n            nn.Dropout(p=dropout),\n            nn.Linear(1024, 512),\n            nn.ReLU(True),\n            nn.Dropout(p=dropout),\n            nn.Linear(512, num_classes),\n        )\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        # N x 3 x 224 x 224\n        x = self.features(x)\n        # N x 512 x 7 x 7\n        x = torch.flatten(x, start_dim=1)\n        # N x 512*7*7\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                # nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n\ndef make_features(cfg: list):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == \"M\":\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            layers += [conv2d, nn.ReLU(True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\ncfgs = {\n    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\ndef vgg(model_name=\"vgg11\", **kwargs):\n    try:\n        cfg = cfgs[model_name]\n    except:\n        print(\"Warning: model number {} not in cfgs dict!\".format(model_name))\n        exit(-1)\n    model = VGG(make_features(cfg), **kwargs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:48.903858Z","iopub.execute_input":"2023-03-20T18:02:48.904698Z","iopub.status.idle":"2023-03-20T18:02:48.933554Z","shell.execute_reply.started":"2023-03-20T18:02:48.904656Z","shell.execute_reply":"2023-03-20T18:02:48.932153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Configurations","metadata":{}},{"cell_type":"code","source":"# \"cuda\" only when GPUs are available.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Initialize a model, and put it on the device specified.\n# model = Classifier().to(device)\n# VGG\n# model_name = \"vgg11\"\n# model = vgg(model_name=model_name, num_classes=11, init_weights=True)\n# model.to(device)\n\n# densenet\nmodel = models.densenet201(weights=None)\nin_features = model.classifier.in_features\nmodel.classifier = nn.Linear(in_features,11)\nmodel.to(device)\n# The number of batch size.\nbatch_size = 64\n\n# The number of training epochs.\nn_epochs = 100\n\n# If no improvement in 'patience' epochs, early stop.\npatience = 20\n\n# For the classification task, we use cross-entropy as the measurement of performance.\ncriterion = nn.CrossEntropyLoss()\n\n# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n\nPATH = \"/kaggle/input/best-model-densenet/best_model (2).pt\"\ncheckpoint = torch.load(PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\nmodel.train()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:48.935670Z","iopub.execute_input":"2023-03-20T18:02:48.936321Z","iopub.status.idle":"2023-03-20T18:02:49.544492Z","shell.execute_reply.started":"2023-03-20T18:02:48.936283Z","shell.execute_reply":"2023-03-20T18:02:49.543459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader","metadata":{}},{"cell_type":"code","source":"# Construct train and valid datasets.\n# The argument \"loader\" tells how torchvision reads the data.\n# train_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/train\", tfm=train_tfm)\n# train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n# valid_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/valid\", tfm=test_tfm)\n# valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:49.547093Z","iopub.execute_input":"2023-03-20T18:02:49.547707Z","iopub.status.idle":"2023-03-20T18:02:49.554731Z","shell.execute_reply.started":"2023-03-20T18:02:49.547668Z","shell.execute_reply":"2023-03-20T18:02:49.553625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Time Augmentation\n_dataset_dir = \"/kaggle/input/ml2023spring-hw3\"\n# Construct datasets.\n# The argument \"loader\" tells how torchvision reads the data.\n\ntrain_path=os.path.join(_dataset_dir,\"train\")\nvalidation_path=os.path.join(_dataset_dir,\"valid\")\nall_file= sorted([os.path.join(train_path,x) for x in os.listdir(train_path) if x.endswith(\".jpg\")])\nall_file+= sorted([os.path.join(validation_path,x) for x in os.listdir(validation_path) if x.endswith(\".jpg\")])\n\ntrain_size=int(len(all_file)*0.8)\nvalidation_size=len(all_file)-train_size\ntrain_data,validation_data=data.random_split(all_file, [train_size,validation_size])\nprint(train_size)\nprint(validation_size)\n\n\ntrain_set=FoodDataset(os.path.join(_dataset_dir,\"train\"), tfm=train_tfm,files=train_data)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set=FoodDataset_test(os.path.join(_dataset_dir,\"valid\"), tfm=train_tfm,files=validation_data)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n'''\ntrain_set = FoodDataset(os.path.join(_dataset_dir,\"training\"), tfm=train_tfm)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\nvalid_set = FoodDataset(os.path.join(_dataset_dir,\"validation\"), tfm=test_tfm)\nvalid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n'''","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:49.556540Z","iopub.execute_input":"2023-03-20T18:02:49.557635Z","iopub.status.idle":"2023-03-20T18:02:49.629046Z","shell.execute_reply.started":"2023-03-20T18:02:49.557600Z","shell.execute_reply":"2023-03-20T18:02:49.627955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Start Training","metadata":{}},{"cell_type":"code","source":"# Initialize trackers, these are not parameters and should not be changed\nstale = 0\nbest_acc = 0\n\nplt_train_loss = []\nplt_valid_loss = []\n\nfor epoch in range(n_epochs):\n\n    # ---------- Training ----------\n    # Make sure the model is in train mode before training.\n    model.train()\n\n    # These are used to record information in training.\n    train_loss = []\n    train_accs = []\n\n    for batch in tqdm(train_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, labels = batch\n        \n        # check Data Augmentation\n        #plt.imshow(imgs[0].permute(1,2,0))\n        #plt.show()\n        #imgs = imgs.half()\n        #print(imgs.shape,labels.shape)\n\n        # Forward the data. (Make sure data and model are on the same device.)\n        logits = model(imgs.to(device))\n\n        # Calculate the cross-entropy loss.\n        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n        loss = criterion(logits, labels.to(device))\n\n        # Gradients stored in the parameters in the previous step should be cleared out first.\n        optimizer.zero_grad()\n\n        # Compute the gradients for parameters.\n        loss.backward()\n\n        # Clip the gradient norms for stable training.\n        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n\n        # Update the parameters with computed gradients.\n        optimizer.step()\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        train_loss.append(loss.item())\n        train_accs.append(acc)\n        \n    train_loss = sum(train_loss) / len(train_loss)\n    train_acc = sum(train_accs) / len(train_accs)\n    plt_train_loss.append(train_loss)\n\n    # Print the information.\n    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n\n    # ---------- Validation ----------\n    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n    model.eval()\n\n    # These are used to record information in validation.\n    valid_loss = []\n    valid_accs = []\n\n    # Iterate the validation set by batches.\n    for batch in tqdm(valid_loader):\n\n        # A batch consists of image data and corresponding labels.\n        imgs, tt, labels = batch\n        #imgs = imgs.half()\n\n        # We don't need gradient in validation.\n        # Using torch.no_grad() accelerates the forward process.\n        with torch.no_grad():\n            logits = model(imgs.to(device))\n\n        # We can still compute the loss (but not the gradient).\n        loss = criterion(logits, labels.to(device))\n\n        # Compute the accuracy for current batch.\n        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n\n        # Record the loss and accuracy.\n        valid_loss.append(loss.item())\n        valid_accs.append(acc)\n        #break\n\n    # The average loss and accuracy for entire validation set is the average of the recorded values.\n    valid_loss = sum(valid_loss) / len(valid_loss)\n    valid_acc = sum(valid_accs) / len(valid_accs)\n    \n    plt_valid_loss.append(valid_loss)\n\n    # Print the information.\n    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # update logs\n    if valid_acc > best_acc:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n    else:\n        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n\n\n    # save models\n    if valid_acc > best_acc:\n        print(f\"Best model found at epoch {epoch}, saving model\")\n        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n        best_acc = valid_acc\n        best_epoch = epoch\n        best_acc = valid_acc\n        save_loss = valid_loss\n        stale = 0\n    else:\n        stale += 1\n        if stale > patience:\n            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n            x = list(range(0, epoch+1))\n            fig, ax = plt.subplots(figsize = (10, 10))\n            ax.plot(x, plt_train_loss)\n            ax.plot(x, plt_valid_loss)\n            ax.legend(['Train Loss', 'Valid Loss'])\n            break\n        \n    # train_loss and valid_loss line chart\n    if epoch == n_epochs-1:\n        x = list(range(0, n_epochs))\n        fig, ax = plt.subplots(figsize = (10, 10))\n        ax.plot(x, plt_train_loss)\n        ax.plot(x, plt_valid_loss)\n        ax.legend(['Train Loss', 'Valid Loss'])\n        \nPATH = \"best_model.pt\"\ntorch.save({\n            'epoch': best_epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_acc': best_acc,\n            'loss': save_loss,\n            }, PATH)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:02:49.630818Z","iopub.execute_input":"2023-03-20T18:02:49.631540Z","iopub.status.idle":"2023-03-20T18:08:45.977761Z","shell.execute_reply.started":"2023-03-20T18:02:49.631497Z","shell.execute_reply":"2023-03-20T18:08:45.976544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataloader for test","metadata":{}},{"cell_type":"code","source":"# Construct test datasets.\n# The argument \"loader\" tells how torchvision reads the data.\ntest_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/test\", tfm=test_tfm)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:08:45.979252Z","iopub.execute_input":"2023-03-20T18:08:45.982671Z","iopub.status.idle":"2023-03-20T18:08:46.074790Z","shell.execute_reply.started":"2023-03-20T18:08:45.982640Z","shell.execute_reply":"2023-03-20T18:08:46.073812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing and generate prediction CSV","metadata":{}},{"cell_type":"code","source":"# model_best = Classifier().to(device)\n\n# VGG\n# model_name = \"vgg11\"\n# model_best = vgg(model_name=model_name, num_classes=11, init_weights=True)\n# model_best.to(device)\n\n# dendenet\nmodel_best = models.densenet201(weights=None)\nin_featurest = model_best.classifier.in_features\nmodel_best.classifier = nn.Linear(in_featurest,11)\nmodel_best.to(device)\n\n\nmodel_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\nmodel_best.eval()\nprediction = []\nwith torch.no_grad():\n    for data,_ in tqdm(test_loader):\n        test_pred = model_best(data.to(device))\n        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n        prediction += test_label.squeeze().tolist()","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:08:46.076309Z","iopub.execute_input":"2023-03-20T18:08:46.076704Z","iopub.status.idle":"2023-03-20T18:09:32.807510Z","shell.execute_reply.started":"2023-03-20T18:08:46.076665Z","shell.execute_reply":"2023-03-20T18:09:32.806435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create test csv\ndef pad4(i):\n    return \"0\"*(4-len(str(i)))+str(i)\ndf = pd.DataFrame()\ndf[\"Id\"] = [pad4(i) for i in range(len(test_set))]\ndf[\"Category\"] = prediction\ndf.to_csv(\"submission.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2023-03-20T18:09:32.809059Z","iopub.execute_input":"2023-03-20T18:09:32.810868Z","iopub.status.idle":"2023-03-20T18:09:32.837903Z","shell.execute_reply.started":"2023-03-20T18:09:32.810828Z","shell.execute_reply":"2023-03-20T18:09:32.836910Z"},"trusted":true},"execution_count":null,"outputs":[]}]}