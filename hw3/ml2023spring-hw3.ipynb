{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## HW3 Image Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-20T12:20:59.978491Z","iopub.status.busy":"2023-03-20T12:20:59.977990Z","iopub.status.idle":"2023-03-20T12:21:00.264968Z","shell.execute_reply":"2023-03-20T12:21:00.263825Z","shell.execute_reply.started":"2023-03-20T12:20:59.978448Z"},"trusted":true},"outputs":[],"source":["# check GPU type.\n","!nvidia-smi"]},{"cell_type":"markdown","metadata":{},"source":["### Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-20T12:21:00.267496Z","iopub.status.busy":"2023-03-20T12:21:00.267068Z","iopub.status.idle":"2023-03-20T12:21:00.271446Z","shell.execute_reply":"2023-03-20T12:21:00.270580Z","shell.execute_reply.started":"2023-03-20T12:21:00.267450Z"},"trusted":true},"outputs":[],"source":["_exp_name = \"sample\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-20T12:21:00.273166Z","iopub.status.busy":"2023-03-20T12:21:00.272718Z","iopub.status.idle":"2023-03-20T12:21:02.898434Z","shell.execute_reply":"2023-03-20T12:21:02.897551Z","shell.execute_reply.started":"2023-03-20T12:21:00.273140Z"},"trusted":true},"outputs":[],"source":["# Import necessary packages.\n","import numpy as np\n","import pandas as pd\n","import torch\n","import os\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from PIL import Image\n","# \"ConcatDataset\" and \"Subset\" are possibly useful when doing semi-supervised learning.\n","from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n","from torchvision.datasets import DatasetFolder, VisionDataset\n","# This is for the progress bar.\n","from tqdm.auto import tqdm\n","import random\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-20T12:21:02.900863Z","iopub.status.busy":"2023-03-20T12:21:02.900399Z","iopub.status.idle":"2023-03-20T12:21:02.908857Z","shell.execute_reply":"2023-03-20T12:21:02.907934Z","shell.execute_reply.started":"2023-03-20T12:21:02.900837Z"},"trusted":true},"outputs":[],"source":["myseed = 6666  # set a random seed for reproducibility\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(myseed)\n","torch.manual_seed(myseed)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(myseed)"]},{"cell_type":"markdown","metadata":{},"source":["### Transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-20T12:21:02.910303Z","iopub.status.busy":"2023-03-20T12:21:02.909957Z","iopub.status.idle":"2023-03-20T12:21:02.919214Z","shell.execute_reply":"2023-03-20T12:21:02.918301Z","shell.execute_reply.started":"2023-03-20T12:21:02.910268Z"},"trusted":true},"outputs":[],"source":["# Normally, We don't need augmentations in testing and validation.\n","# All we need here is to resize the PIL image and transform it into Tensor.\n","test_tfm = transforms.Compose([\n","    transforms.Resize((128, 128)),\n","    transforms.ToTensor(),\n","])\n","\n","# However, it is also possible to use augmentation in the testing phase.\n","# You may use train_tfm to produce a variety of images and then test using ensemble methods\n","\n","train_tfm = transforms.Compose([\n","    # Resize the image into a fixed shape (height = width = 128)\n","    \n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.ColorJitter(brightness=(0.7, 1.3), contrast=(\n","        0.7, 1.3), saturation=(0.7, 1.3)),\n","    transforms.RandomRotation(45),\n","    transforms.Resize((128, 128)),\n","    # You may add some transforms here.\n","    \n","    # ToTensor() should be the last one of the transforms.\n","    transforms.ToTensor(),\n","])"]},{"cell_type":"markdown","metadata":{},"source":["### Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-20T12:21:02.921399Z","iopub.status.busy":"2023-03-20T12:21:02.920964Z","iopub.status.idle":"2023-03-20T12:21:02.933050Z","shell.execute_reply":"2023-03-20T12:21:02.931505Z","shell.execute_reply.started":"2023-03-20T12:21:02.921369Z"},"trusted":true},"outputs":[],"source":["class FoodDataset(Dataset):\n","\n","    def __init__(self,path,tfm=test_tfm,files = None):\n","        super(FoodDataset).__init__()\n","        self.path = path\n","        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n","        if files != None:\n","            self.files = files\n","            \n","        self.transform = tfm\n","  \n","    def __len__(self):\n","        return len(self.files)\n","  \n","    def __getitem__(self,idx):\n","        fname = self.files[idx]\n","        im = Image.open(fname)\n","        im = self.transform(im)\n","        \n","        try:\n","            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n","        except:\n","            label = -1 # test has no label\n","            \n","        return im,label"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class FoodDataset_test(Dataset):\n","\n","    def __init__(self,path,tfm=test_tfm,tfm2=train_tfm,files = None):\n","        super(FoodDataset_test).__init__()\n","        self.path = path\n","        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n","        if files != None:\n","            self.files = files\n","        print(f\"One {path} sample\",self.files[0])\n","        self.transform = tfm\n","        self.train_transform = tfm2\n","  \n","    def __len__(self):\n","        return len(self.files)\n","  \n","    def __getitem__(self,idx):\n","        fname = self.files[idx]\n","        im = Image.open(fname)\n","        \n","        x=[]\n","        y=self.train_transform(im)\n","        for i in range(5):\n","            x.append(self.train_transform(im))\n","        #im = self.data[idx]\n","        im = self.transform(im)\n","        try:\n","            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n","        except:\n","            label = -1 # test has no label\n","        return im,x,label"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-20T12:21:02.934718Z","iopub.status.busy":"2023-03-20T12:21:02.934339Z","iopub.status.idle":"2023-03-20T12:21:02.949639Z","shell.execute_reply":"2023-03-20T12:21:02.948645Z","shell.execute_reply.started":"2023-03-20T12:21:02.934685Z"},"trusted":true},"outputs":[],"source":["class Classifier(nn.Module):\n","    def __init__(self):\n","        super(Classifier, self).__init__()\n","        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n","        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n","        # input 維度 [3, 128, 128]\n","        self.cnn = nn.Sequential(\n","            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n","\n","            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n","\n","            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n","\n","            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n","            \n","            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n","        )\n","        self.fc = nn.Sequential(\n","            nn.Linear(512*4*4, 1024),\n","            nn.ReLU(),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 11)\n","        )\n","\n","    def forward(self, x):\n","        out = self.cnn(x)\n","        out = out.view(out.size()[0], -1)\n","        return self.fc(out)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class VGG(nn.Module):\n","    def __init__(self, features, num_classes=11, init_weights=False, dropout: float = 0.5):\n","        super(VGG, self).__init__()\n","        self.features = features\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 4 * 4, 1024),\n","            nn.ReLU(True),\n","            nn.Dropout(p=dropout),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(True),\n","            nn.Dropout(p=dropout),\n","            nn.Linear(512, num_classes),\n","        )\n","        if init_weights:\n","            self._initialize_weights()\n","\n","    def forward(self, x):\n","        # N x 3 x 224 x 224\n","        x = self.features(x)\n","        # N x 512 x 7 x 7\n","        x = torch.flatten(x, start_dim=1)\n","        # N x 512*7*7\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                nn.init.xavier_uniform_(m.weight)\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.xavier_uniform_(m.weight)\n","                # nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","\n","def make_features(cfg: list):\n","    layers = []\n","    in_channels = 3\n","    for v in cfg:\n","        if v == \"M\":\n","            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","        else:\n","            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","            layers += [conv2d, nn.ReLU(True)]\n","            in_channels = v\n","    return nn.Sequential(*layers)\n","\n","\n","cfgs = {\n","    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","\n","def vgg(model_name=\"vgg11\", **kwargs):\n","    try:\n","        cfg = cfgs[model_name]\n","    except:\n","        print(\"Warning: model number {} not in cfgs dict!\".format(model_name))\n","        exit(-1)\n","    model = VGG(make_features(cfg), **kwargs)\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["### Configurations"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-20T12:21:02.952527Z","iopub.status.busy":"2023-03-20T12:21:02.951658Z","iopub.status.idle":"2023-03-20T12:21:03.158054Z","shell.execute_reply":"2023-03-20T12:21:03.156670Z","shell.execute_reply.started":"2023-03-20T12:21:02.952489Z"},"trusted":true},"outputs":[],"source":["# \"cuda\" only when GPUs are available.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Initialize a model, and put it on the device specified.\n","# model = Classifier().to(device)\n","model_name = \"vgg11\"\n","model = vgg(model_name=model_name, num_classes=11, init_weights=True)\n","model.to(device)\n","# The number of batch size.\n","batch_size = 64\n","\n","# The number of training epochs.\n","n_epochs = 50\n","\n","# If no improvement in 'patience' epochs, early stop.\n","patience = 10\n","\n","# For the classification task, we use cross-entropy as the measurement of performance.\n","criterion = nn.CrossEntropyLoss()\n","\n","# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n","\n","PATH = \"/kaggle/input/best-model-tta/best_model (1).pt\"\n","checkpoint = torch.load(PATH)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n","loss = checkpoint['loss']\n","model.train()"]},{"cell_type":"markdown","metadata":{},"source":["### Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-20T12:21:03.159974Z","iopub.status.busy":"2023-03-20T12:21:03.159332Z","iopub.status.idle":"2023-03-20T12:21:03.403428Z","shell.execute_reply":"2023-03-20T12:21:03.402447Z","shell.execute_reply.started":"2023-03-20T12:21:03.159936Z"},"trusted":true},"outputs":[],"source":["# Test Time Augmentation\n","_dataset_dir = \"/kaggle/input/ml2023spring-hw3\"\n","# Construct datasets.\n","# The argument \"loader\" tells how torchvision reads the data.\n","\n","train_path=os.path.join(_dataset_dir,\"train\")\n","validation_path=os.path.join(_dataset_dir,\"valid\")\n","all_file= sorted([os.path.join(train_path,x) for x in os.listdir(train_path) if x.endswith(\".jpg\")])\n","all_file+= sorted([os.path.join(validation_path,x) for x in os.listdir(validation_path) if x.endswith(\".jpg\")])\n","\n","train_size=int(len(all_file)*0.8)\n","validation_size=len(all_file)-train_size\n","train_data,validation_data=data.random_split(all_file, [train_size,validation_size])\n","print(train_size)\n","print(validation_size)\n","\n","\n","train_set=FoodDataset(os.path.join(_dataset_dir,\"train\"), tfm=train_tfm,files=train_data)\n","train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n","valid_set=FoodDataset_test(os.path.join(_dataset_dir,\"valid\"), tfm=train_tfm,files=validation_data)\n","valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Start Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-20T12:21:03.406672Z","iopub.status.busy":"2023-03-20T12:21:03.406154Z","iopub.status.idle":"2023-03-20T12:21:42.095701Z","shell.execute_reply":"2023-03-20T12:21:42.094159Z","shell.execute_reply.started":"2023-03-20T12:21:03.406643Z"},"trusted":true},"outputs":[],"source":["# Initialize trackers, these are not parameters and should not be changed\n","stale = 0\n","best_acc = 0\n","\n","plt_train_loss = []\n","plt_valid_loss = []\n","\n","for epoch in range(n_epochs):\n","\n","    # ---------- Training ----------\n","    # Make sure the model is in train mode before training.\n","    model.train()\n","\n","    # These are used to record information in training.\n","    train_loss = []\n","    train_accs = []\n","\n","    for batch in tqdm(train_loader):\n","\n","        # A batch consists of image data and corresponding labels.\n","        imgs, labels = batch\n","        \n","        # check Data Augmentation\n","        #plt.imshow(imgs[0].permute(1,2,0))\n","        #plt.show()\n","        #imgs = imgs.half()\n","        #print(imgs.shape,labels.shape)\n","\n","        # Forward the data. (Make sure data and model are on the same device.)\n","        logits = model(imgs.to(device))\n","\n","        # Calculate the cross-entropy loss.\n","        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n","        loss = criterion(logits, labels.to(device))\n","\n","        # Gradients stored in the parameters in the previous step should be cleared out first.\n","        optimizer.zero_grad()\n","\n","        # Compute the gradients for parameters.\n","        loss.backward()\n","\n","        # Clip the gradient norms for stable training.\n","        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n","\n","        # Update the parameters with computed gradients.\n","        optimizer.step()\n","\n","        # Compute the accuracy for current batch.\n","        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n","\n","        # Record the loss and accuracy.\n","        train_loss.append(loss.item())\n","        train_accs.append(acc)\n","        \n","    train_loss = sum(train_loss) / len(train_loss)\n","    train_acc = sum(train_accs) / len(train_accs)\n","    plt_train_loss.append(train_loss)\n","\n","    # Print the information.\n","    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n","\n","    # ---------- Validation ----------\n","    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n","    model.eval()\n","\n","    # These are used to record information in validation.\n","    valid_loss = []\n","    valid_accs = []\n","\n","    # Iterate the validation set by batches.\n","    for batch in tqdm(valid_loader):\n","\n","        # A batch consists of image data and corresponding labels.\n","        imgs, tt, labels = batch\n","        #imgs = imgs.half()\n","\n","        # We don't need gradient in validation.\n","        # Using torch.no_grad() accelerates the forward process.\n","        with torch.no_grad():\n","            logits = model(imgs.to(device))\n","\n","        # We can still compute the loss (but not the gradient).\n","        loss = criterion(logits, labels.to(device))\n","\n","        # Compute the accuracy for current batch.\n","        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n","\n","        # Record the loss and accuracy.\n","        valid_loss.append(loss.item())\n","        valid_accs.append(acc)\n","        #break\n","\n","    # The average loss and accuracy for entire validation set is the average of the recorded values.\n","    valid_loss = sum(valid_loss) / len(valid_loss)\n","    valid_acc = sum(valid_accs) / len(valid_accs)\n","    \n","    plt_valid_loss.append(valid_loss)\n","\n","    # Print the information.\n","    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n","\n","\n","    # update logs\n","    if valid_acc > best_acc:\n","        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n","            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n","    else:\n","        with open(f\"./{_exp_name}_log.txt\",\"a\"):\n","            print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n","\n","\n","    # save models\n","    if valid_acc > best_acc:\n","        print(f\"Best model found at epoch {epoch}, saving model\")\n","        torch.save(model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n","        best_acc = valid_acc\n","        best_epoch = epoch\n","        best_acc = valid_acc\n","        save_loss = valid_loss\n","        stale = 0\n","    else:\n","        stale += 1\n","        if stale > patience:\n","            print(f\"No improvment {patience} consecutive epochs, early stopping\")\n","            x = list(range(0, epoch+1))\n","            fig, ax = plt.subplots(figsize = (10, 10))\n","            ax.plot(x, plt_train_loss)\n","            ax.plot(x, plt_valid_loss)\n","            ax.legend(['Train Loss', 'Valid Loss'])\n","            break\n","        \n","    # train_loss and valid_loss line chart\n","    if epoch == n_epochs-1:\n","        x = list(range(0, n_epochs))\n","        fig, ax = plt.subplots(figsize = (10, 10))\n","        ax.plot(x, plt_train_loss)\n","        ax.plot(x, plt_valid_loss)\n","        ax.legend(['Train Loss', 'Valid Loss'])\n","        \n","PATH = \"best_model.pt\"\n","torch.save({\n","            'epoch': best_epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'best_acc': best_acc,\n","            'loss': save_loss,\n","            }, PATH)"]},{"cell_type":"markdown","metadata":{},"source":["### Dataloader for test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-03-20T12:21:42.096770Z","iopub.status.idle":"2023-03-20T12:21:42.097191Z","shell.execute_reply":"2023-03-20T12:21:42.096997Z","shell.execute_reply.started":"2023-03-20T12:21:42.096973Z"},"trusted":true},"outputs":[],"source":["# Construct test datasets.\n","# The argument \"loader\" tells how torchvision reads the data.\n","test_set = FoodDataset(\"/kaggle/input/ml2023spring-hw3/test\", tfm=test_tfm)\n","test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Testing and generate prediction CSV"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-03-20T12:21:42.098882Z","iopub.status.idle":"2023-03-20T12:21:42.099611Z","shell.execute_reply":"2023-03-20T12:21:42.099358Z","shell.execute_reply.started":"2023-03-20T12:21:42.099326Z"},"trusted":true},"outputs":[],"source":["# model_best = Classifier().to(device)\n","model_name = \"vgg11\"\n","model_best = vgg(model_name=model_name, num_classes=11, init_weights=True)\n","model_best.to(device)\n","model_best.load_state_dict(torch.load(f\"{_exp_name}_best.ckpt\"))\n","model_best.eval()\n","prediction = []\n","with torch.no_grad():\n","    for data,_ in tqdm(test_loader):\n","        test_pred = model_best(data.to(device))\n","        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n","        prediction += test_label.squeeze().tolist()mo"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-03-20T12:21:42.100925Z","iopub.status.idle":"2023-03-20T12:21:42.101356Z","shell.execute_reply":"2023-03-20T12:21:42.101148Z","shell.execute_reply.started":"2023-03-20T12:21:42.101125Z"},"trusted":true},"outputs":[],"source":["#create test csv\n","def pad4(i):\n","    return \"0\"*(4-len(str(i)))+str(i)\n","df = pd.DataFrame()\n","df[\"Id\"] = [pad4(i) for i in range(len(test_set))]\n","df[\"Category\"] = prediction\n","df.to_csv(\"submission.csv\",index = False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
